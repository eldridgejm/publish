"""
publish
=======

A tool to build and publish certain artifacts at certain times.

`publish` was desgined specifically for the automatic publication of course
materials, such as homeworks, lecture slides, etc.


Terminology
-----------

An **artifact** is a file -- usually one that is generated by some build process.

A **publication** is a coherent group of one or more artifacts and their metadata.

A **collection** is a group of publications which all satisfy the same **schema**.

A **schema** is a set of constraints on a publication's artifacts and metadata.

This establishes a **collection -> publication -> artifact hierarchy**: each
artifact belongs to exactly one publication, and each publication belongs to
exactly one collection.

An example of such a hierarchy is the following: all homeworks in a course form
a collection. Each publication within the collection is an individual
homework. Each publication may have several artifacts, such as the PDF of the
problem set, the PDF of the solutions, and a .zip containing the homework's
data.

An artifact may have a **release time**, before which it will not be built or published.
Likewise, entire publications can have release times, too.

Discovering, Building, and Publishing
-------------------------------------

When run as a script, this package follows a three step process of discovering,
building, and publishing artifacts. 

In the **discovery** step, the script constructs a collection -> publication ->
artifact hierarchy by recursively searching an input directory for artifacts.

In the **build** step, the script builds every artifact whose release time has passed.

In the **publish** step, the script copies every released artifact to an output
directory. 


Discovery
~~~~~~~~~

In the discovery step, the **input directory** is recursively searched for collections,
publications, and artifacts.

A collection is defined by creating a file named ``collections.yaml`` in a directory.
The contents of the file describe the artifacts and metadata that are required
of each of the publications within the collection. For instance: 

.. code-block:: yaml

    # <input_directory>/homeworks/collection.yaml

    schema:
        required_artifacts:
            - homework.pdf
            - solution.pdf

        optional_artifacts:
            - template.zip

        metadata_schema:
            name: 
                type: string
            due:
                type: datetime
            released:
                type: date

The file above specifies that publications must have ``homework.pdf`` and
``solution.pdf`` artifacts, and may or may not have a ``template.zip``
artifact. The publications must also have *name*, *due*, and *released* fields
in their metadata with the listed types. The metadata specification is given in a form
recognizable by the *cerberus* Python package.


A publication and its artifacts are defined by creating a ``publish.yaml`` file
in the directory containing the publication. For instance, the file below
describes how and when to build two artifacts named ``homework.pdf`` and ``solution.pdf``,
along with metadata:

.. code-block:: yaml

    # <input_directory>/homeworks/01-intro/publish.yaml

    metadata:
        name: Homework 01
        due: 2020-09-04 23:59:00
        released: 2020-09-01

    artifacts:
        homework.pdf:
            recipe: make homework
        solution.pdf:
            file: ./build/solution.pdf
            recipe: make solution
            release_time: 1 day after metadata.due
            ready: false
            missing_ok: false

The ``file`` field tells *publish* where the file will appear when the recipe
is run.  is omitted, its value is assumed to be the artifact's key -- for
instance, ``homework.pdf``'s ``file`` field is simply ``homework.pdf``.

The ``release_time`` field provides the artifact's release time. It can be a
specific datetime in ISO 8601 format, like ``2020-09-18 17:00:00``, or a
*relative* date of the form "<number> (hour|day)[s]{0,1} (before|after)
metadata.<field>", in which case the date will be calculated relative to the
metadata field.  The field it refers to must be a datetime.

The ``ready`` field is a manual override which prevents the artifact from
being built and published before it is ready. If not provided, the artifact
is assumed to be ready.

THe ``missing_ok`` field is a boolean which, if ``false``, causes an error to
be raised if the artifact's file is missing after the build. This is the
default behavior.  If set to ``true``, no error is raised. This can be useful
when the artifact file is manually placed in the directory and it is
undesirable to repeatedly edit ``publish.yaml`` to add the artifact.

Publications may also have ``release_time`` and ``ready`` attributes. If these
are provided they will take precedence over the attributes of an individual
artifact in the publication. The release time of the publication can be used
to control when its metadata becomes available -- before the release time,
the publication in effect does not exist.

The file hierarchy determines which publications belong to which collections.
If a publication file is placed in a directory that is a descendent of a
directory containing a collection file, the publication will be placed in that
collection and its contents will be validated against the collection's schema.
Publications which are not under a directory containing a ``collection.yaml``
are placed into a "default" collection with no schema. They may contain any
number of artifacts and metadata keys.

Collections, publications, and artifacts all have **keys** which locate them
within the hierarchy. These keys are inferred from their position in the
filesystem. For example, a collection file placed at
``<input_directory>/homeworks/collection.yaml`` will create a collection keyed
"homeworks". A publication within the collection at
``<input_directory>/homeworks/01-intro/publish.yaml`` will be keyed "01-intro".
The keys of the artifacts are simply their keys within the ``publish.yaml``
file.


Building
~~~~~~~~

Once all collections, publications, and artifacts have been discovered, the
script moves to the build phase.

Artifacts are built by running the command given in the artifact's `recipe`
field within the directory containing the artifact's ``publication.yaml`` file.
Different artifacts should have "orthogonal" build processes so that the order
in which the artifacts are built is inconsequential.

If an error occurs during any build the entire process is halted and the
program returns without continuing on to the publish phase. An error is
considered to occur if the build process returns a nonzero error code, or if
the artifact file is missing after the recipe is run.


Publishing
~~~~~~~~~~

In the publish phase, all published artifacts -- that is, those which are ready
and whose release date has passed -- are copied to an **output directory**.
Additionally, a JSON file containing information about the collection ->
publication -> artifact hierarchy is placed at the root of the output
directory.

Artifacts are copied to a location within the output directory according to the
following "formula":

.. code-block:: text

    <output_directory>/<collection_key>/<publication_key>/<artifact_key>

For instance, an artifact keyed ``homework.pdf`` in the ``01-intro`` publication
of the ``homeworks`` collection will be copied to::

    <output_directory>/homeworks/01-intro/homework.pdf

An artifact which has not been released will not be copied, even if the
artifact file exists.

*publish* will create a JSON file named ``<output_directory>/published.json``.
This file contains nested dictionaries describing the structure of the
collection -> publication -> artifact hierarchy. 

For example, the below code will load the JSON file and print the path of a published
artifact relative to the output directory, as well as a publication's metadata.

.. code-block:: python

    >>> import json
    >>> d = json.load(open('published.json'))
    >>> d['collections']['homeworks']['publications']['01-intro']['artifacts']['homework.pdf']['path']
    homeworks/01-intro/homework.pdf
    >>> d['collections']['homeworks']['publications']['01-intro']['metadata']['due']
    2020-09-10 23:59:00

Only those publications and artifacts which have been published appear in the
JSON file. In particular, if an artifact has not reached its release time, it
will be missing from the JSON representation entirely.

"""

from collections import deque, namedtuple, defaultdict
from textwrap import dedent
import abc
import argparse
import copy
import datetime
import json
import pathlib
import re
import shutil
import subprocess
import typing
import yaml
import cerberus

# constants
# --------------------------------------------------------------------------------------

__version__ = (0, 1, 5)

# the file used to define a collection
COLLECTION_FILE = "collection.yaml"

# the file used to define a publication and its artifacts
PUBLICATION_FILE = "publish.yaml"


# exceptions
# --------------------------------------------------------------------------------------


class Error(Exception):
    """Generic error."""


class ValidationError(Error):
    """Publication does not satisfy schema."""


class DiscoveryError(Error):
    """A configuration file is not valid."""

    def __init__(self, msg, path):
        self.path = path
        self.msg = msg

    def __str__(self):
        return f"Error reading {self.path}: {self.msg}"


class BuildError(Error):
    """Problem while building the artifact."""


# types
# --------------------------------------------------------------------------------------


class Artifact:
    """Base class for all artifact types."""


class UnbuiltArtifact(Artifact, typing.NamedTuple):
    """The inputs needed to build an artifact.

    Attributes
    ----------
    workdir : pathlib.Path
        Absolute path to the working directory used to build the artifact.
    file : str
        Path (relative to the workdir) of the file produced by the build.
    recipe : Union[str, None]
        Command used to build the artifact. If None, no command is necessary.
    release_time: Union[datetime.datetime, None]
        Time/date the artifact should be made public. If None, it is always available.
    ready : bool
        Whether or not the artifact is ready for publication. Default: True.
    missing_ok : bool
        If True and the file is missing after building, then no error is raised and the
        result of the build is `None`.

    """

    workdir: pathlib.Path
    file: str
    recipe: str = None
    release_time: datetime.datetime = None
    ready: bool = True
    missing_ok: bool = False


class BuiltArtifact(Artifact, typing.NamedTuple):
    """The results of building an artifact.

    Attributes
    ----------
    workdir : pathlib.Path
        Absolute path to the working directory used to build the artifact.
    file : str
        Path (relative to the workdir) of the file produced by the build.
    returncode : int
        The build process's return code. If None, there was no process.
    stdout : str
        The build process's stdout. If None, there was no process.
    stderr : str
        The build process's stderr. If None, there was no process.

    """

    workdir: pathlib.Path
    file: str
    returncode: int = None
    stdout: str = None
    stderr: str = None


class PublishedArtifact(Artifact, typing.NamedTuple):
    """A published artifact.

    Attributes
    ----------
    path : str
        The path to the artifact's file relative to the output directory.

    """

    path: str


def _artifact_from_dict(dct):
    """Infers the artifact type from the dictionary and performs conversion."""
    if "recipe" in dct:
        type_ = UnbuiltArtifact
    elif "returncode" in dct:
        type_ = BuiltArtifact
    else:
        type_ = PublishedArtifact

    return type_(**dct)


# the following are "Internal Nodes" of the collection -> publication ->
# artifact hierarchy. they all have _children attributes and _deep_asdict
# and _replace_children methods>


class Publication(typing.NamedTuple):
    """A publication.

    Attributes
    ----------
    artifacts : Dict[str, Artifact]
        The artifacts contained in the publication.
    metadata: Dict[str, Any]
        The metadata dictionary.

    """

    metadata: typing.Mapping[str, typing.Any]
    artifacts: typing.Mapping[str, Artifact]
    ready: bool = True
    release_time: datetime.datetime = None

    def _deep_asdict(self):
        """A dictionary representation of the publication and its children."""
        return {
            "metadata": self.metadata,
            "artifacts": {k: a._asdict() for (k, a) in self.artifacts.items()},
        }

    @classmethod
    def _deep_fromdict(cls, dct):
        return cls(
            metadata=dct["metadata"],
            artifacts={
                k: _artifact_from_dict(d) for (k, d) in dct["artifacts"].items()
            },
        )

    @property
    def _children(self):
        return self.artifacts

    def _replace_children(self, new_children):
        return self._replace(artifacts=new_children)


class Collection(typing.NamedTuple):
    """A collection.

    Attributes
    ----------
    schema : Schema
        The schema used to validate the publications within the collection.
    publications : Dict[str, Publication]
        The publications contained in the collection.

    """

    schema: "Schema"
    publications: typing.Mapping[str, Publication]

    def _deep_asdict(self):
        """A dictionary representation of the collection and its children."""
        return {
            "schema": self.schema._asdict(),
            "publications": {
                k: p._deep_asdict() for (k, p) in self.publications.items()
            },
        }

    @classmethod
    def _deep_fromdict(cls, dct):
        return cls(
            schema=Schema(**dct["schema"]),
            publications={
                k: Publication._deep_fromdict(d)
                for (k, d) in dct["publications"].items()
            },
        )

    @property
    def _children(self):
        return self.publications

    def _replace_children(self, new_children):
        return self._replace(publications=new_children)


class Universe(typing.NamedTuple):
    """Container of all collections.

    Attributes
    ----------

    collections : Dict[str, Collection]
        The collections.

    """

    collections: typing.Mapping[str, Collection]

    @property
    def _children(self):
        return self.collections

    def _replace_children(self, new_children):
        return self._replace(collections=new_children)

    def _deep_asdict(self):
        """A dictionary representation of the universe and its children."""
        return {
            "collections": {k: p._deep_asdict() for (k, p) in self.collections.items()},
        }

    @classmethod
    def _deep_fromdict(cls, dct):
        return cls(
            collections={
                k: Collection._deep_fromdict(d) for (k, d) in dct["collections"].items()
            },
        )


class Schema(typing.NamedTuple):
    """Rules governing publications.

    Attributes
    ----------
    required_artifacts : typing.Collection[str]
        Names of artifacts that publications must contain.
    optional_artifacts : typing.Collection[str], optional
        Names of artifacts that publication are permitted to contain. Default: empty
        list.
    allow_unspecified_artifacts : bool, optional
        Is it permissible for a publication to have unknown artifacts? Default: False.
    metadata_schema : Mapping[str, Any], optional
        A dictionary describing a schema used to validate publication metadata. In the
        style of cerberus. If None, no validation will be performed. Default: None.

    """

    required_artifacts: typing.Collection[str]
    optional_artifacts: typing.Collection[str] = None
    metadata_schema: typing.Mapping[str, typing.Mapping] = None
    allow_unspecified_artifacts: bool = False


# validation
# --------------------------------------------------------------------------------------


def validate(publication: Publication, against: Schema):
    """Make sure that a publication satisfies the schema.

    This checks the publication's metadata dictionary against
    ``against.metadata_schema``. Verifies that all required artifacts are
    provided, and that no unknown artifacts are given (unless
    ``schema.allow_unspecified_artifacts == True``).

    Parameters
    ----------
    publication : Publication
        A fully-specified publication.
    against : Schema
        A schema for validating the publication.

    Raises
    ------
    ValidationError
        If the publication does not satisfy the schema's constraints.

    """
    schema = against

    # make an iterable default for optional artifacts
    if schema.optional_artifacts is None:
        schema = schema._replace(optional_artifacts={})

    # if there is a metadata schema, enforce it
    if schema.metadata_schema is not None:
        validator = cerberus.Validator(schema.metadata_schema, require_all=True)
        validated = validator.validated(publication.metadata)
        if validated is None:
            raise ValidationError(f"Invalid metadata. {validator.errors}")

    # ensure that all required artifacts are present
    required = set(schema.required_artifacts)
    optional = set(schema.optional_artifacts)
    provided = set(publication.artifacts)
    extra = provided - (required | optional)

    if required - provided:
        raise ValidationError(f"Required artifacts omitted: {required - provided}.")

    if extra and not schema.allow_unspecified_artifacts:
        raise ValidationError(f"Unknown artifacts provided: {provided - optional}.")


# discovery
# --------------------------------------------------------------------------------------


def read_collection_file(path):
    """Read a :class:`Collection` from a yaml file.

    Parameters
    ----------
    path : pathlib.Path
        Path to the collection file.

    Returns
    -------
    Collection
        The collection object with no attached publications.

    Notes
    -----
    The file should have one key, "schema", whose value is a dictionary with
    the following keys/values:

    - required_artifacts
        A list of artifacts names that are required
    - optional_artifacts [optional]
        A list of artifacts that are optional. If not provided, the default value of []
        (empty list) will be used.
    - metadata_schema [optional]
        A dictionary describing a schema for validating publication metadata.  The
        dictionary should deserialize to something recognized by the cerberus package.
        If not provided, the default value of None will be used.
    - allow_unspecified_artifacts [optional]
        Whether or not to allow unspecified artifacts in the publications.
        Default: False.

    """
    with path.open() as fileobj:
        contents = yaml.load(fileobj, Loader=yaml.Loader)

    # define the structure of the collections file. we require only the
    # 'required_artifacts' field.
    validator = cerberus.Validator(
        {
            "schema": {
                "schema": {
                    "required_artifacts": {
                        "type": "list",
                        "schema": {"type": "string"},
                        "required": True,
                    },
                    "optional_artifacts": {
                        "type": "list",
                        "schema": {"type": "string"},
                        "default": [],
                    },
                    "metadata_schema": {
                        "type": "dict",
                        "required": False,
                        "nullable": True,
                        "default": None,
                    },
                    "allow_unspecified_artifacts": {
                        "type": "boolean",
                        "default": False,
                    },
                },
            }
        },
        require_all=True,
    )

    # validate and normalize
    validated_contents = validator.validated(contents)

    if validated_contents is None:
        raise DiscoveryError(str(validator.errors), path)

    # make sure that the metadata schema is valid
    if validated_contents["schema"]["metadata_schema"] is not None:
        try:
            cerberus.Validator(validated_contents["schema"]["metadata_schema"])
        except Exception as exc:
            raise DiscoveryError("Invalid metadata schema.", path)

    schema = Schema(**validated_contents["schema"])
    return Collection(schema=schema, publications={})


def _parse_release_time(s, metadata):
    """Convert a string like '1 day after metadata.due' to a datetime.

    Also works with hours: "3 hours after metadata.due"
    
    Parameters
    ----------
    s
        A time, maybe in the format of a string, but also possibly None or a datetime.
        If it isn't a string, the function simply returns s. This is useful if
        s is None, for instance.
    metadata : dict
        The metadata dictionary used to look up the reference date.

    Returns
    -------
    datetime.datetime or None
        The release time.

    Raises
    ------
    ValidationError
        If the relative date string format is incorrect.

    """
    if not isinstance(s, str):
        return s

    short_match = re.match(r"metadata\.(\w+)$", s)
    long_match = re.match(
        r"^(\d+) (hour|day)[s]{0,1} (after|before) metadata\.(\w+)$", s
    )

    if short_match:
        [variable] = short_match.groups()
        factor = 1
        number = 0
        hours_or_days = "day"
    elif long_match:
        number, hours_or_days, before_or_after, variable = long_match.groups()
        factor = -1 if before_or_after == "before" else 1
    else:
        raise ValueError("Invalid relative date string.")

    if variable not in metadata or not isinstance(
        metadata[variable], datetime.datetime
    ):
        raise ValueError(f"Invalid reference variable '{variable}'. Not a datetime.")

    if hours_or_days == "hour":
        timedelta_kwargs = {"hours": factor * int(number)}
    else:
        timedelta_kwargs = {"days": factor * int(number)}

    delta = datetime.timedelta(**timedelta_kwargs)
    return metadata[variable] + delta


def read_publication_file(path, schema=None):
    """Read a :class:`Publication` from a yaml file.

    Parameters
    ----------
    path : pathlib.Path
        Path to the collection file.
    schema : Optional[Schema]
        A schema for validating the publication. Default: None, in which case the
        publication's metadata are not validated.

    Returns
    -------
    Publication
        The publication.

    Raises
    ------
    ValidationError
        If ``schema`` is provided and the publication metadata is not valid.

    Notes
    -----

    The file should have a "metadata" key whose value is a dictionary
    of metadata. It should also have an "artifacts" key whose value is a
    dictionary mapping artifact names to artifact definitions.

    Optionally, the file can have a "release_time" key providing a time at
    which the publication should be considered released. It may also have
    a "ready" key; if this is False, the publication will not be considered
    released.

    If the ``schema`` argument is not provided, only very basic validation is
    performed by this function. Namely, the metadata schema and
    required/optional artifacts are not enforced. See the :func:`validate`
    function for validating these aspects of the publication. If the schema is
    provided, :func:`validate` is called as a convenience.


    """
    with path.open() as fileobj:
        contents = yaml.load(fileobj, Loader=yaml.Loader)

    # we'll just do a quick check of the file structure first. validating the metadata
    # schema and checking that the right artifacts are provided will be done later
    quick_schema = {
        "ready": {"type": "boolean", "default": True, "nullable": True},
        "release_time": {
            "type": ["datetime", "string"],
            "default": None,
            "nullable": True,
        },
        "metadata": {"type": "dict", "required": False, "default": {}},
        "artifacts": {
            "required": True,
            "valuesrules": {
                "schema": {
                    "file": {"type": "string", "default": None, "nullable": True},
                    "recipe": {"type": "string", "default": None, "nullable": True},
                    "ready": {"type": "boolean", "default": True, "nullable": True},
                    "missing_ok": {"type": "boolean", "default": False},
                    "release_time": {
                        "type": ["datetime", "string"],
                        "default": None,
                        "nullable": True,
                    },
                }
            },
        },
    }

    # validate and normalize the contents
    validator = cerberus.Validator(quick_schema, require_all=True)
    validated = validator.validated(contents)

    if validated is None:
        raise DiscoveryError(str(validator.errors), path)

    metadata = validated["metadata"]

    # convert each artifact to an Artifact object
    artifacts = {}
    for key, definition in validated["artifacts"].items():
        # handle relative release times
        try:
            definition["release_time"] = _parse_release_time(
                definition["release_time"], metadata
            )
        except ValueError as exc:
            raise DiscoveryError(str(exc), path)

        # if no file is provided, use the key
        if definition["file"] is None:
            definition["file"] = key

        artifacts[key] = UnbuiltArtifact(workdir=path.parent.absolute(), **definition)

    # handle publication release time
    try:
        release_time = _parse_release_time(validated["release_time"], metadata)
    except ValueError as exc:
        raise DiscoveryError(str(exc), path)

    publication = Publication(
        metadata=metadata,
        artifacts=artifacts,
        ready=validated["ready"],
        release_time=release_time,
    )

    if schema is not None:
        validate(publication, against=schema)

    return publication


# discovery: discover()
# --------------------------------------------------------------------------------------


class DiscoverCallbacks:
    """Callbacks used in :func:`discover`. Defaults do nothing."""

    def on_collection(self, path):
        """When a collection is discovered.

        Parameters
        ----------
        path : pathlib.Path
            The path of the collection file.

        """

    def on_publication(self, path):
        """When a publication is discovered.

        Parameters
        ----------
        path : pathlib.Path
            The path of the publication file.

        """

    def on_skip(self, path):
        """When a directory is skipped.

        Parameters
        ----------
        path : pathlib.Path
            The path of the directory to be skipped.

        """


# represents a node in the BFS used in discover()
_BFSNode = namedtuple("Node", "path parent_collection parent_collection_path")


def _discover_bfs(
    initial_node, make_collection, make_publication, skip_directories, callbacks
):
    """Execute a BFS to find collections/publications in the filesystem."""

    queue = deque([initial_node])
    while queue:
        # the current directory, parent collection, and it's path
        (path, parent_collection, parent_collection_path) = node = queue.popleft()

        is_collection = (node.path / COLLECTION_FILE).is_file()
        is_publication = (node.path / PUBLICATION_FILE).is_file()

        if is_collection and is_publication:
            raise DiscoveryError("Cannot be both a publication and a collection.", path)

        if is_collection:
            collection = make_collection(node)

            # now that we're in a new collection, the parent has changed
            parent_collection = collection
            parent_collection_path = path

        if is_publication:
            make_publication(node)

        for subpath in path.iterdir():

            if subpath.name in skip_directories:
                callbacks.on_skip(subpath)
                continue

            if subpath.is_dir():
                node = _BFSNode(
                    path=subpath,
                    parent_collection=parent_collection,
                    parent_collection_path=parent_collection_path,
                )
                queue.append(node)


def discover(
    input_directory: pathlib.Path,
    skip_directories: typing.Optional[typing.Collection[str]] = None,
    callbacks: typing.Optional[DiscoverCallbacks] = None,
) -> Universe:
    """Discover the collections and publications in the filesystem.

    Parameters
    ----------
    input_directory
        The path to the directory that will be recursively searched.
    skip_directories
        A collection of directory names that should be skipped if discovered.
        If None, no directories will be skipped.
    callbacks : Optional[DiscoverCallbacks]
        Callbacks to be invoked during the discovery. If omitted, no callbacks
        are executed. See :class:`DiscoverCallbacks` for the possible callbacks
        and their arguments.

    Returns
    -------
    Universe
        The collections and the nested publications and artifacts, contained in
        a :class:`Universe` instance.

    """
    if skip_directories is None:
        skip_directories = set()

    if callbacks is None:
        callbacks = DiscoverCallbacks()

    # the collection publications are added to if they belong to no other collection
    default_schema = Schema(
        required_artifacts=[], metadata_schema=None, allow_unspecified_artifacts=True,
    )
    default_collection = Collection(schema=default_schema, publications={})

    # by default, we have just the default collection; we'll discover more
    collections = {"default": default_collection}

    # we will run a BFS to discover the collection -> publication -> archive
    # hierarchy. each BFS node will be a triple of the current directory, the
    # parent collection, and the path to the parent collection's directory
    initial_node = _BFSNode(
        path=input_directory,
        parent_collection=default_collection,
        parent_collection_path=input_directory,
    )

    # to simplify the code, our BFS function will outsource the creation and validation
    # of our new collection/publication to the below callbacks functions

    def make_collection(node: _BFSNode):
        """Called when a new collection is discovered. Creates/returns collection."""
        path, parent_collection, parent_collection_path = node

        # ensure no nested collections
        if parent_collection is not default_collection:
            raise DiscoveryError("Nested collection found.", path)

        # create the collection
        collection_file = path / COLLECTION_FILE
        new_collection = read_collection_file(collection_file)

        # add it to the rest of the collections
        key = str(path.relative_to(input_directory))
        collections[key] = new_collection

        # callback
        callbacks.on_collection(collection_file)

        # return the new collection
        return collections[key]

    def make_publication(node: _BFSNode):
        """Called when a new publication is discovered."""
        path, parent_collection, parent_collection_path = node

        # read the publication file
        publication_file = path / PUBLICATION_FILE

        try:
            publication = read_publication_file(publication_file, schema=parent_collection.schema)
        except ValidationError as exc:
            raise DiscoveryError(str(exc), publication_file)

        # add the publication to the parent collection
        key = str(path.relative_to(parent_collection_path))
        parent_collection.publications[key] = publication

        # callbacks
        callbacks.on_publication(publication_file)

    # run the BFS; this will populate the `collections` dictionary
    _discover_bfs(
        initial_node, make_collection, make_publication, skip_directories, callbacks
    )

    return Universe(collections)


# filter_nodes()
# --------------------------------------------------------------------------------------


class FilterCallbacks:
    def on_hit(self, x):
        """On an artifact match."""

    def on_miss(self, x):
        """On an artifact miss."""


def filter_nodes(parent, predicate, remove_empty_nodes=False, callbacks=None):
    """Remove nodes from a Universe/Collection/Publication.

    Parameters
    ----------
    parent
        The root of the tree.
    predicate : Callable[[node], bool]
        A function which takes in a node and returns True/False whether it
        should be kept.
    remove_empty_nodes : bool
        Whether nodes without children should be removed (True) or preserved
        (False). Default: False.
    
    Returns
    -------
    type(parent)
        An object of the same type as the parent, but wth all filtered nodes
        removed. Furthermore, if a node has no children after filtering, it
        is removed.

    """
    # bottom up -- by the time the predicate is applied to publication, its artifacts
    # have been filtered

    if isinstance(parent, (UnbuiltArtifact, BuiltArtifact, PublishedArtifact)):
        return parent

    new_children = {}
    for child_key, child in parent._children.items():
        new_child = filter_nodes(
            child, predicate, remove_empty_nodes=remove_empty_nodes, callbacks=callbacks
        )
        is_artifact = isinstance(
            new_child, (UnbuiltArtifact, BuiltArtifact, PublishedArtifact)
        )
        if is_artifact or (not remove_empty_nodes) or new_child._children:
            new_children[child_key] = new_child

    new_children = {k: v for (k, v) in new_children.items() if predicate(k, v)}

    return parent._replace_children(new_children)


# building
# --------------------------------------------------------------------------------------


class BuildCallbacks:
    """Callbacks used by :func:`build`"""

    def on_build(self, key, node):
        """Called when building a collection/publication/artifact."""

    def on_too_soon(self, artifact: UnbuiltArtifact):
        """Called when it is too soon to release the artifact."""

    def on_not_ready(self, artifact: UnbuiltArtifact):
        """Called when the artifact is not ready."""

    def on_missing(self, artifact: UnbuiltArtifact):
        """Called when the artifact file is missing, but missing is OK."""

    def on_recipe(self, artifact: UnbuiltArtifact):
        """Called when artifact is being built using its recipe."""

    def on_success(self, artifact: BuiltArtifact):
        """Called when the build succeeded."""


def _build_artifact(
    artifact,
    *,
    ignore_release_time=False,
    now=datetime.datetime.now,
    verbose=False,
    run=subprocess.run,
    exists=pathlib.Path.exists,
    callbacks=BuildCallbacks(),
):
    """Build an artifact using its recipe.

    Parameters
    ----------
    artifact : Artifact
        The artifact to build.
    ignore_release_time : bool
        If True, the release time of the artifact will be ignored, and it will
        be built anyways. Default: False.

    Returns
    -------
    Optional[BuiltArtifact]
        A summary of the build results. The result is `None` if the build time
        is in the future, or if the file was not created and missing_ok is
        True.

    """
    output = BuiltArtifact(workdir=artifact.workdir, file=artifact.file)

    if (
        not ignore_release_time
        and artifact.release_time is not None
        and artifact.release_time > now()
    ):
        callbacks.on_too_soon(artifact)
        return None

    if not artifact.ready:
        callbacks.on_not_ready(artifact)
        return None

    if artifact.recipe is None:
        stdout = None
        stderr = None
        returncode = None
    else:
        callbacks.on_recipe(artifact)

        kwargs = {
            "cwd": artifact.workdir,
        }
        if not verbose:
            kwargs["stdout"] = subprocess.PIPE
            kwargs["stderr"] = subprocess.PIPE

        proc = run(artifact.recipe, shell=True, **kwargs)

        if proc.returncode:
            msg = "There was a problem while building the artifact"
            if proc.stderr is not None:
                msg += f":\n{proc.stderr.decode()}"
            raise BuildError(msg)

        returncode = proc.returncode
        stdout = None if proc.stdout is None else proc.stdout.decode()
        stderr = None if proc.stderr is None else proc.stderr.decode()

    filepath = artifact.workdir / artifact.file
    if not exists(filepath):
        if artifact.missing_ok:
            callbacks.on_missing(artifact)
            return None
        else:
            raise BuildError(f"Artifact file {filepath} does not exist.")

    output = output._replace(returncode=returncode, stdout=stdout, stderr=stderr)
    callbacks.on_success(output)
    return output


def build(
    parent: typing.Union[Universe, Collection, Publication, UnbuiltArtifact],
    *,
    ignore_release_time=False,
    verbose=False,
    now=datetime.datetime.now,
    run=subprocess.run,
    exists=pathlib.Path.exists,
    callbacks=None,
):
    """Build a universe/collection/publication/artifact.

    Parameters
    ----------
    parent : Union[Universe, Collection, Publication, UnbuiltArtifact]
        The thing to build. Operates recursively, so if given a
        :class:`Universe`, for instance, will build all of the artifacts
        within.
    ignore_release_time : bool
        If ``True``, all artifacts will be built, even if their release time
        has not yet passed.
    callbacks : Optional[BuildCallbacks]
        Callbacks to be invoked during the build. If omitted, no callbacks
        are executed. See :class:`BuildCallbacks` for the possible callbacks
        and their arguments.

    Returns
    -------
    Optional[type(parent)]
        A copy of the parent where each leaf artifact is replaced with
        an instance of :class:`BuiltArtifact`. If the thing to be built is not
        built due to being unreleased, ``None`` is returned.

    Note
    ----
    If a publication or artifact is not yet released, either due to its release
    time being in the future or because it is marked as not ready, its recipe will
    not be run. If the parent node is a publication or artifact that is not
    built, the result of this function is None. If the parent node is a collection
    or universe, all of the unbuilt publications and artifacts within are
    recursively removed from the tree.

    """
    if callbacks is None:
        callbacks = BuildCallbacks()

    kwargs = dict(
        ignore_release_time=ignore_release_time,
        now=now,
        run=run,
        verbose=verbose,
        exists=exists,
        callbacks=callbacks,
    )

    if isinstance(parent, UnbuiltArtifact):
        return _build_artifact(parent, **kwargs)

    if isinstance(parent, Publication):
        if not parent.ready:
            callbacks.on_not_ready(parent)
            return None

        if (
            not ignore_release_time
            and parent.release_time is not None
            and parent.release_time > now()
        ):
            callbacks.on_too_soon(parent)
            return None

    # recursively build the children
    new_children = {}
    for child_key, child in parent._children.items():
        callbacks.on_build(child_key, child)
        result = build(child, **kwargs)
        # if a node is not built (perhaps due to it not being ready), the
        # result is None. this next conditional prevents such nodes from
        # appearing in the tree
        if result is not None:
            new_children[child_key] = result
    return parent._replace_children(new_children)


# publishing
# --------------------------------------------------------------------------------------


class PublishCallbacks:
    def on_copy(self, src, dst):
        """Called when copying a file."""

    def on_publish(self, key, node):
        """When publish is called on a node."""


def _publish_artifact(built_artifact, outdir, filename, callbacks):

    # actually copy the artifact
    full_dst = outdir / filename
    full_dst.parent.mkdir(parents=True, exist_ok=True)
    full_src = built_artifact.workdir / built_artifact.file
    callbacks.on_copy(full_src, full_dst)
    shutil.copy(full_src, full_dst)

    return PublishedArtifact(path=full_dst.relative_to(outdir))


def publish(parent, outdir, prefix="", callbacks=None):
    """Publish a universe/collection/publication/artifact by copying it.

    Parameters
    ----------
    parent : Union[Universe, Collection, Publication, BuiltArtifact]
        The thing to publish.
    outdir : pathlib.Path
        Path to the output directory where artifacts will be copied.
    prefix : str
        String to prepend between output directory path and the keys of the
        children. If the thing being published is a :class:`BuiltArtifact`,
        this is simply the filename.
    callbacks : PublishCallbacks
        Callbacks to be invoked during the publication. If omitted, no
        callbacks are executed. See :class:`PublishCallbacks` for the possible
        callbacks and their arguments.

    Returns
    -------
    type(parent)
        A copy of the parent, but with all leaf artifact nodes replace by
        :class:`PublishedArtifact` instances. Artifacts which have not yet
        been released are still converted to PublishedArtifact, but their ``path``
        is set to ``None``.
    
    Notes
    -----
    The prefix is build up recursively, so that calling this function on a
    universe will publish each artifact to 
    ``<prefix><collection_key>/<publication_key>/<artifact_key>``

    """
    if callbacks is None:
        callbacks = PublishCallbacks()

    if isinstance(parent, BuiltArtifact):
        return _publish_artifact(parent, outdir, prefix, callbacks)

    new_children = {}
    for child_key, child in parent._children.items():
        callbacks.on_publish(child_key, child)
        new_prefix = pathlib.Path(prefix) / child_key
        new_children[child_key] = publish(child, outdir, new_prefix, callbacks)

    return parent._replace_children(new_children)


# serialization
# --------------------------------------------------------------------------------------


def serialize(node):
    """Serialize the universe/collection/publication/artifact to JSON.

    Parameters
    ----------
    node : Union[Universe, Collection, Publication, Artifact]
        The thing to serialize as JSON.

    Returns
    -------
    str
        The object serialized as JSON.

    """

    def converter(o):
        return str(o)

    if isinstance(node, Artifact):
        dct = node._asdict()
    else:
        dct = node._deep_asdict()

    return json.dumps(dct, default=converter, indent=4)


def _convert_to_time(s):
    converters = [datetime.date.fromisoformat, datetime.datetime.fromisoformat]
    for converter in converters:
        try:
            return converter(s)
        except ValueError:
            continue
    else:
        raise ValueError("Not a time.")


def deserialize(s):
    """Reconstruct a universe/collection/publication/artifact from JSON.

    Parameters
    ----------
    s : str
        The JSON to deserialize.

    Returns
    -------
    Universe/Collection/Publication/Artifact
        The reconstructed object; its type is inferred from the string.

    """
    # we need to pass a hook to json.loads in order to automatically convert
    # datestring to date/datetime objects
    def hook(pairs):
        """Hook for json.loads to convert date/time-like values."""
        d = {}
        for k, v in pairs:
            if isinstance(v, str):
                try:
                    d[k] = _convert_to_time(v)
                except ValueError:
                    d[k] = v
            else:
                d[k] = v
        return d

    dct = json.loads(s, object_pairs_hook=hook)

    # infer what we're reconstructing
    if "collections" in dct:
        type_ = Universe
        children_key = "collections"
    elif "publications" in dct:
        type_ = Collection
        children_key = "publications"
    elif "artifacts" in dct:
        type_ = Publication
        children_key = "artifacts"
    else:
        return _artifact_from_dict(dct)

    return type_._deep_fromdict(dct)


# cli
# --------------------------------------------------------------------------------------


def _arg_directory(s):
    path = pathlib.Path(s)
    if not path.is_dir():
        raise argparse.ArgumentTypeError("Not a directory.")
    return path


def _arg_output_directory(s):
    path = pathlib.Path(s)

    if not path.exists():
        path.mkdir(parents=True)
        return path

    return _arg_directory(path)


def cli(argv=None):
    """The command line interface.

    Parameters
    ----------
    argv : List[str]
        A list of command line arguments. If None, the arguments will be read from the
        command line passed to the process by the shell.
    now : Callable[[], datetime.datetime]
        A callable producing the current datetime. This is useful when testing, as it
        allows you to inject a fixed, known time.

    """
    parser = argparse.ArgumentParser()
    parser.add_argument("input_directory", type=_arg_directory)
    parser.add_argument("output_directory", type=_arg_output_directory)
    parser.add_argument(
        "--skip-directories",
        type=str,
        nargs="+",
        help="directories that will be ignored during discovery",
    )
    parser.add_argument(
        "--ignore-release-time",
        action="store_true",
        help="if provided, all artifacts will be built and published regardless of release time",
    )
    parser.add_argument(
        "--artifact-filter",
        type=str,
        default=None,
        help="artifacts will be built and published only if their key matches this string",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        default=False,
        help="let stdout and stderr through when building artifacts",
    )
    parser.add_argument("--now",)
    args = parser.parse_args(argv)

    if args.now is None:
        now = datetime.datetime.now
    else:
        try:
            n_days = int(args.now)
            _now = datetime.datetime.now() + datetime.timedelta(days=n_days)
        except ValueError:
            _now = datetime.datetime.fromisoformat(args.now)

        def now():
            return _now

    # construct callbacks for printing information to the screen. start with
    # helper functions for formatting terminal output

    def _header(message):
        return "\u001b[1m" + message + "\u001b[0m"

    def _normal(message):
        return message

    def _body(message):
        return "\u001b[2m" + message + "\u001b[0m"

    def _warning(message):
        return "\u001b[33m" + message + "\u001b[0m"

    def _success(message):
        return "\u001b[32m" + message + "\u001b[0m"

    def _error(message):
        return "\u001b[31m" + message + "\u001b[0m"

    # the callbacks

    class CLIDiscoverCallbacks(DiscoverCallbacks):
        def on_publication(self, path):
            publication_name = str(path.parent)
            print(f"{_normal(publication_name)}")

        def on_skip(self, path):
            relpath = path.relative_to(args.input_directory)
            print(_warning(f"Skipping directory {relpath}"))

    class CLIBuildCallbacks(BuildCallbacks):
        def on_build(self, key, node):
            if isinstance(node, UnbuiltArtifact):
                relative_workdir = node.workdir.relative_to(
                    args.input_directory.absolute()
                )
                path = relative_workdir / key
                print(_normal(str(path)), end="")

        def on_too_soon(self, node):
            if isinstance(node, UnbuiltArtifact):
                tabs = "    "
            else:
                tabs = ""

            msg = (
                f"{tabs}Release time {node.release_time} has not yet been reached. "
                "Skipping."
            )
            print(_warning(msg))

        def on_missing(self, node):
            print(_warning(" file missing, but missing_ok=True"))

        def on_not_ready(self, node):
            msg = f"not ready → skipping"

            if isinstance(node, UnbuiltArtifact):
                print(_warning(f" {msg}"))

            else:
                for key, artifact in node.artifacts.items():
                    relative_workdir = artifact.workdir.relative_to(
                        args.input_directory.absolute()
                    )
                    path = relative_workdir / key
                    print(str(path) + " " + _warning(msg))

        def on_success(self, output):
            print(_success(" build was successful ✓"))

    class CLIFilterCallbacks(FilterCallbacks):
        def on_miss(self, x):
            key = f"{x.collection_key}/{x.publication_key}/{x.artifact_key}"
            print(_warning(f"\tRemoving {key}"))

        def on_hit(self, x):
            key = f"{x.collection_key}/{x.publication_key}/{x.artifact_key}"
            print(_success(f"\tKeeping {key}"))

    class CLIPublishCallbacks(PublishCallbacks):
        def on_copy(self, src, dst):
            src = src.relative_to(args.input_directory.absolute())
            dst = dst.relative_to(args.output_directory)
            msg = f"<input_directory>/{src} to <output_directory>/{dst}."
            print(_normal(msg))

    # begin the discover -> build -> publish process

    print()
    print(_header("Discovered publications:"))

    discovered = discover(
        args.input_directory,
        skip_directories=args.skip_directories,
        callbacks=CLIDiscoverCallbacks(),
    )

    if args.artifact_filter is not None:
        # filter out artifacts whose keys do not match this string

        def keep(k, v):
            if not isinstance(v, UnbuiltArtifact):
                return True
            else:
                return k == args.artifact_filter

        discovered = filter_nodes(
            discovered, keep, remove_empty_nodes=True, callbacks=CLIFilterCallbacks()
        )

    print()
    print(_header("Building:"))

    built = build(
        discovered,
        callbacks=CLIBuildCallbacks(),
        ignore_release_time=args.ignore_release_time,
        verbose=args.verbose,
        now=now,
    )

    print()
    print(_header("Copying:"))
    published = publish(built, args.output_directory, callbacks=CLIPublishCallbacks())

    # serialize the results
    with (args.output_directory / "published.json").open("w") as fileobj:
        fileobj.write(serialize(published))


if __name__ == "__main__":
    cli()
