"""
publish
=======

A tool to build and publish artifacts.


Terminology
-----------

An **artifact** is a file â€” usually one that is generated by some build process.

A **publication** is a coherent group of one or more artifacts and their metadata.

A **schema** is a set of constraints on a publication's artifacts and metadata.

A **collection** is a group of publications which all satisfy the same **schema**.

This establishes a **collection -> publication -> artifact hierarchy**: each
artifact belongs to exactly one publication, and each publication belongs to
exactly one collection.

An example of such a hierarchy is the following: all homeworks in a course form
a collection.  Each publication within the collection is an individual
homework. Each publication may have several artifacts, such as the PDF of the
problem set, the PDF of the solutions, and a .zip containing the homework's
data.


Discovering, Building, and Publishing
-------------------------------------

When run as a script, this package follows a three step process of discovering, building,
and publishing artifacts.

In the discovery step, the script builds a collection -> publication ->
artifact hierarchy by recursively searching the filesystem -- more on this
below. The result of this step is one or more collections. Each collection and publication
is given a key 

In the build step, the script builds every artifact that was found in the
previous step, provided that the artifact's release_time has passed.

In the publish step, the script copies every artifact to 


Collection and Publication Files
--------------------------------

This package adopts the following convention for defining a collection ->
publication -> artifact hierarchy via the filesystem.

A publication and its artifacts are defined in a `publish.yaml` file. For instance,
the file below describes how and when to build two artifacts named "homework"
and "solution".

    # publish.yaml

    metadata:
        name: Homework 01
        due: 2020-09-04 23:59:00
        released: 2020-09-01

    artifacts:
        homework:
            file: ./homework.pdf
            recipe: make homework
        solution:
            file: ./solution.pdf
            recipe: make solution
            release_time: 1 days after metadata.foo

Collections are defined in a `collection.yaml` file. The file provides a schema used
to validate any publications that will be placed within the collection. For example:

    schema:
        required_artifacts:
            - homework
            - solution

        optional_artifacts:
            - template

        metadata_schema:
            name: 
                type: string
            due:
                type: datetime
            released:
                type: date

This package builds a collection -> publication -> artifact hierarchy by
recursively searching a directory tree. If a `collection.yaml` file is found,
all publications found in descendant directories are validated with respect to
the collection's schema and placed within the collection. If there is no
`collection.yaml` above a publication, it is placed within a "default"
collection and no schema validation is performed.





"""

from collections import deque, namedtuple
from textwrap import dedent
import copy
import datetime
import pathlib
import re
import shutil
import subprocess
import typing
import yaml

import cerberus


COLLECTION_FILE = "collection.yaml"
PUBLICATION_FILE = "publish.yaml"


class Error(Exception):
    """Generic error."""


class SchemaError(Error):
    """Publication does not satisfy schema."""


class InvalidFileError(Error):
    """A configuration file is not valid."""

    def __init__(self, msg, path):
        self.path = path
        self.msg = msg

    def __str__(self):
        return f'Error reading {self.path}: {msg}'


class BuildError(Error):
    """Problem while building the artifact."""


class ArtifactInputs(typing.NamedTuple):
    """An artifact."""

    # the working directory used to build the artifact
    workdir: pathlib.Path

    # the path to the file that is the result of the build, relative to the workdir
    file: str

    # the command used to build the artifact. if None, no command is necessary
    recipe: str = None

    # time the artifact should be made public. if None, it is always available
    release_time: datetime.datetime = None


class ArtifactOutputs(typing.NamedTuple):
    """An artifact's build results."""

    # the working directory used to build the artifact
    workdir: pathlib.Path

    # the path to the file that is the result of the build, relative to the workdir
    file: str

    # whether or not the artifact is released
    is_released: bool

    # the stdout of the build. if None, the build didn't happen
    proc: subprocess.CompletedProcess


class Publication(typing.NamedTuple):
    """A publication."""

    # a dictionary of metadata
    metadata: typing.Mapping[str, typing.Any]

    # a dictionary of artifacts
    artifacts: typing.Mapping[str, ArtifactInputs]

    def _deep_asdict(self):
        return {
            "metadata": self.metadata,
            "artifacts": {k: a._asdict() for (k, a) in self.artifacts.items()},
        }


class Collection(typing.NamedTuple):
    """A collection."""

    # the schema that publications should follow
    schema: "Schema"

    # a dictionary of publications
    publications: typing.Mapping[str, Publication]

    def _deep_asdict(self):
        return {
            "schema": self.schema._asdict(),
            "publications": {
                k: p._deep_asdict() for (k, p) in self.publications.items()
            },
        }


class Schema(typing.NamedTuple):
    """Rules governing publications.

    Attributes
    ----------
    required_artifacts : Collection[str]
        Names of artifacts that publications must contain.
    optional_artifacts : Collection[str]
        Names of artifacts that publication are permitted to contain. Default: empty
        list.
    allow_unspecified_artifacts : bool
        Is it permissible for a publication to have unknown artifacts? Default: False.
    metadata_schema : dict
        A dictionary describing a schema used to validate publication metadata. In the
        style of cerberus. If None, no validation will be performed. Default: None.

    """

    required_artifacts: typing.Collection[str]
    optional_artifacts: typing.Collection[str] = None
    metadata_schema: typing.Mapping[str, typing.Mapping] = None
    allow_unspecified_artifacts: bool = False


def validate(publication, against):
    """Make sure that a publication fits within the collection.

    Check's the publication's metadata dictionary against
    collection.metadata_schema. Verifies that all required artifacts are provided,
    and that no unknown artifacts are given (unless
    collection.allow_unspecified_artifacts == True).

    Parameters
    ----------
    publication : Publication
        A fully-specified publication.

    Raises
    ------
    SchemaError
        If the publication does not satisfy the collection's constraints.

    """
    schema = against

    # make an iterable default for optional artifacts
    if schema.optional_artifacts is None:
        schema = schema._replace(optional_artifacts={})

    # if there is a metadata schema, enforce it
    if schema.metadata_schema is not None:
        validator = cerberus.Validator(schema.metadata_schema, require_all=True)
        validated = validator.validated(publication.metadata)
        if validated is None:
            raise SchemaError(f"Invalid metadata. {validator.errors}")

    # ensure that all required artifacts are present
    required = set(schema.required_artifacts)
    optional = set(schema.optional_artifacts)
    provided = set(publication.artifacts)
    extra = provided - (required | optional)

    if required - provided:
        raise SchemaError(f"Required artifacts omitted: {required - provided}.")

    if extra and not schema.allow_unspecified_artifacts:
        raise SchemaError(f"Unknown artifacts provided: {provided - optional}.")


def read_collection_file(path):
    """Read a Collection from a yaml file.

    The file should have one key, "schema", whose value is a dictionary with
    the following keys/values:

    - required_artifacts
        A list of artifacts names that are required
    - optional_artifacts [optional]
        A list of artifacts that are optional. If not provided, the default value of []
        (empty list) will be used.
    - metadata_schema [optional]
        A dictionary describing a schema for validating publication metadata.  The
        dictionary should deserialize to something recognized by the cerberus package.
        If not provided, the default value of None will be used.
    - allow_unspecified_artifacts [optional]
        Whether or not to allow unspecified artifacts in the publications.

    Parameters
    ----------
    path : pathlib.Path
        Path to the collection file.

    Returns
    -------
    Collection
        The collection object with no attached publications.

    """
    with path.open() as fileobj:
        contents = yaml.load(fileobj, Loader=yaml.Loader)

    # define the structure of the collections file. we require only the
    # 'required_artifacts' field.
    validator = cerberus.Validator(
        {
            "schema": {
                "schema": {
                    "required_artifacts": {
                        "type": "list",
                        "schema": {"type": "string"},
                        "required": True,
                    },
                    "optional_artifacts": {
                        "type": "list",
                        "schema": {"type": "string"},
                        "default": [],
                    },
                    "metadata_schema": {
                        "type": "dict",
                        "required": False,
                        "nullable": True,
                        "default": None,
                    },
                    "allow_unspecified_artifacts": {
                        "type": "boolean",
                        "default": False,
                    },
                },
            }
        },
        require_all=True,
    )

    # validate and normalize
    validated_contents = validator.validated(contents)

    if validated_contents is None:
        raise InvalidFileError(str(validator.errors), path)

    # make sure that the metadata schema is valid
    if validated_contents["schema"]["metadata_schema"] is not None:
        try:
            cerberus.Validator(validated_contents["schema"]["metadata_schema"])
        except Exception as exc:
            raise InvalidFileError("Invalid metadata schema.", path)

    schema = Schema(**validated_contents["schema"])
    return Collection(schema=schema, publications={})


def _parse_release_time(s, metadata):
    """Convert a string like '1 day after metadata.due' to a datetime.
    
    Parameters
    ----------
    s
        A time, maybe in the format of a string, but also possibly None or a datetime.
        If it isn't a string, the function does nothing.
    metadata : dict
        The metadata dictionary used to look up the reference date.

    Returns
    -------
    datetime.datetime or None
        The release time.

    Raises
    ------
    SchemaError
        If the relative date string format is incorrect.

    """
    if not isinstance(s, str):
        return s

    short_match = re.match(r"metadata\.(\w+)$", s)
    long_match = re.match(r"(\d) day[s]{0,1} (after|before) metadata\.(\w+)$", s)

    if short_match:
        [variable] = short_match.groups()
        factor = 1
        days = 0
    elif long_match:
        days, before_or_after, variable = long_match.groups()
        factor = -1 if before_or_after == "before" else 1
    else:
        raise ValueError("Invalid relative date string.")

    if variable not in metadata or not isinstance(
        metadata[variable], datetime.datetime
    ):
        raise ValueError(f"Invalid reference variable '{variable}'. Not a datetime.")

    delta = datetime.timedelta(days=factor * int(days))
    return metadata[variable] + delta


def read_publication_file(path):
    """Read a Publication from a yaml file.

    The file should have a "metadata" key whose value is a dictionary obeying
    collection.metadata_schema. It should also have an "artifacts" key whose value is a
    dictionary mapping artifact names to artifact definitions.

    Parameters
    ----------
    path : pathlib.Path
        Path to the collection file.

    Returns
    -------
    Publication
        The publication.

    """
    with path.open() as fileobj:
        contents = yaml.load(fileobj, Loader=yaml.Loader)

    # we'll just do a quick check of the file structure first. validating the metadata
    # schema and checking that the right artifacts are provided will be done later
    schema = {
        "metadata": {"type": "dict", "required": False, "default": {}},
        "artifacts": {
            "required": True,
            "valuesrules": {
                "schema": {
                    "file": {"type": "string"},
                    "recipe": {"type": "string", "default": None, "nullable": True},
                    "release_time": {
                        "type": ["datetime", "string"],
                        "default": None,
                        "nullable": True,
                    },
                }
            },
        },
    }

    # validate and normalize the contents
    validator = cerberus.Validator(schema, require_all=True)
    validated = validator.validated(contents)

    if validated is None:
        raise InvalidFileError(str(validator.errors), path)

    metadata = validated["metadata"]

    # convert each artifact to an Artifact object
    artifacts = {}
    for key, definition in validated["artifacts"].items():
        # handle relative release times
        try:
            definition["release_time"] = _parse_release_time(
                definition["release_time"], metadata
            )
        except ValueError as exc:
            raise InvalidFileError(str(exc), path)
        artifacts[key] = ArtifactInputs(workdir=path.parent.absolute(), **definition)

    return Publication(metadata=metadata, artifacts=artifacts)


def discover(root: pathlib.Path, default_collection=None, ignore=None):
    """Discover the collections and publications in the filesystem.

    Parameters
    ----------
    root : pathlib.Path
        The path that will be recursively explored.
    ignore : Set[str]
        A collection of directory names that will be ignored (not searched).
    default_collection : Collection
        The collection that will be used for publications that do not fall under any
        other collection.

    Returns
    -------
    dict
        A dictionary mapping collection names to Collection instances.

    Notes
    -----

    A Publication is defined by creating a publish.yaml file with metadata and
    instructions for building the publication's artifacts. A Collection is defined by
    creating a collection.yaml file; all publications discovered in a descendant
    directory of the one containing the collection.yaml file will be placed in that
    collection.

    A Collection's key is determined by the relative path from the root of the search to
    the directory containing the collection.yaml. A Publication's key is determined by
    the relative path from the collection containing it to the directory containing
    publish.yaml. For instance, if this function finds a file at
    `<root>/foo/bar/collection.yaml`, it will create a collection with key `foo/bar`.

    Publications which are not under any collection.yaml are placed into a "default"
    collection; this is a collection keyed "default" with lenient metadata schema and no
    restrictions on artifacts.


    """
    if default_collection is None:
        default_schema = Schema(
            required_artifacts=[],
            metadata_schema=None,
            allow_unspecified_artifacts=True,
        )
        default_collection = Collection(schema=default_schema, publications={})

    if ignore is None:
        ignore = set()

    collections = {"default": default_collection}

    # we will run a BFS. each node will be a triple of the current directory, the parent
    # collection, and the path to the parent collection's directory
    Node = namedtuple("Node", "directory parent_collection parent_collection_path")
    initial_node = Node(
        directory=root,
        parent_collection=default_collection,
        parent_collection_path=root,
    )
    queue = deque([initial_node])

    while queue:
        directory, parent_collection, parent_collection_path = queue.popleft()

        collection_file = directory / COLLECTION_FILE
        publication_file = directory / PUBLICATION_FILE

        # if this is a collection directory, create a collection
        if collection_file.is_file():
            # make sure we do not have nested collections
            if parent_collection is not default_collection:
                raise InvalidFileError("Nested collection found.", collection_file)
            # update the parent collection
            parent_collection = read_collection_file(collection_file)
            key = str(directory.relative_to(root))
            collections[key] = parent_collection
            parent_collection_path = directory

        # if this is a publication directory, create a publication
        if publication_file.is_file():
            publication = read_publication_file(publication_file)
            try:
                validate(publication, against=parent_collection.schema)
            except SchemaError as exc:
                raise InvalidFileError(str(exc), publication_file)
            key = str(directory.relative_to(parent_collection_path))
            parent_collection.publications[key] = publication

        # add child nodes to the queue
        for subpath in directory.iterdir():
            if subpath.name in ignore:
                continue
            if subpath.is_dir():
                node = Node(
                    directory=subpath,
                    parent_collection=parent_collection,
                    parent_collection_path=parent_collection_path,
                )
                queue.append(node)

    return collections


def build_artifact(
    artifact,
    ignore_release_time=False,
    now=datetime.datetime.now,
    run=subprocess.run,
    exists=pathlib.Path.exists,
):
    """Build an artifact using its recipe.

    Parameters
    ----------
    artifact : Artifact
        The artifact to build.
    ignore_release_time : bool
        If True, the release time of the artifact will be ignored, and it will
        be built anyways. Default: False.

    Returns
    -------
    ArtifactOutputs
        A summary of the build results.

    """
    build_result = ArtifactOutputs(
        workdir=artifact.workdir, file=artifact.file, is_released=False, proc=None
    )

    if (
        not ignore_release_time
        and artifact.release_time is not None
        and artifact.release_time > now()
    ):
        return build_result

    if artifact.recipe is None:
        proc = None
    else:
        proc = run(
            artifact.recipe,
            shell=True,
            cwd=artifact.workdir,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
        )

        if proc.returncode:
            raise BuildError(
                f"There was a problem while building the artifact: \n{proc.stderr.decode()}"
            )

    filepath = artifact.workdir / artifact.file
    if not exists(filepath):
        raise BuildError(f"Artifact file {filepath} does not exist.")

    return build_result._replace(is_released=True, proc=proc)


def _all_artifacts(collections):
    Location = namedtuple(
        "Location",
        [
            "collection_key",
            "collection",
            "publication_key",
            "publication",
            "artifact_key",
            "artifact",
        ],
    )
    for collection_key, collection in collections.items():
        for publication_key, publication in collection.publications.items():
            for artifact_key, artifact in publication.artifacts.items():
                yield Location(
                    collection_key,
                    collection,
                    publication_key,
                    publication,
                    artifact_key,
                    artifact,
                )


def build(collections):
    """Convenience function to build all of the artifacts in the collections."""
    collections = copy.deepcopy(collections)
    for x in _all_artifacts(collections):
        build_result = build_artifact(x.artifact)
        x.publication.artifacts[x.artifact_key] = build_result
    return collections


def publish(collections, outdir):
    """Copy all of the build results to a destination directory.

    An artifact's destination is determined using the following "formula":

        <collection_key>/<publication_key>/artifact.file

    Since the collection key and publication key can contain slashes, the
    path may be of an arbitrary depth.

    """
    published_collections = copy.deepcopy(collections)
    unreleased = set()

    for x in _all_artifacts(published_collections):
        # if it wasn't released, we shouldn't publish it
        if not x.artifact.is_released:
            unreleased.add((x.collection_key, x.publication_key, x.artifact_key))
            continue

        # actually copy the artifact
        relative_dst = (
            pathlib.Path(x.collection_key) / x.publication_key / x.artifact.file
        )
        full_dst = outdir / relative_dst
        full_dst.parent.mkdir(parents=True, exist_ok=True)
        full_src = x.artifact.workdir / x.artifact.file
        shutil.copy(full_src, full_dst)

        # update the result
        x.publication.artifacts[x.artifact_key] = relative_dst

    # remove all unreleased artifacts
    for (collection_key, publication_key, artifact_key) in unreleased:
        collection = published_collections[collection_key]
        publication = collection.publications[publication_key]
        del publication.artifacts[artifact_key]

    return published_collections
